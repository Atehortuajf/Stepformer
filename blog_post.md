# Stepformer: Generating Rhythm Game Choreography with Music Transformers

**_How can we teach a machine to choreograph dance steps to music?_** This question was first tackled by Donahue et al. in their 2017 paper **“Dance Dance Convolution”** (DDC). They presented a system that listens to a song and produces a Dance Dance Revolution (DDR) step chart – the sequence of arrows and timing that players must follow on a dance pad. DDC treated choreography generation as a prediction problem on **audio spectrograms** using a convolutional neural network (CNN) coupled with an RNN. The result was a pioneering proof-of-concept: neural networks *can* learn to choreograph DDR steps from raw audio. 

However, much has changed in deep learning since 2017. Transformers and sequence models have revolutionized how we handle structured sequential data like text and music. In this research-style exploration, we **review the original DDC approach and its limitations**, then introduce a novel approach that leverages **tokenized music representations** and **Transformer-based sequence-to-sequence (seq2seq) models** (inspired by recent advances like **Music Transformer** and **MT3**) to generate dance choreographies from songs. We will discuss how representing music as sequences of tokens can improve learning, outline possible model architectures, describe necessary data preprocessing, and suggest evaluation metrics. We’ll also look at exciting **applications beyond rhythm games**, such as VR dance generation and AI-assisted artistic choreography. 

**Table of Contents:**
- [Background: Dance Dance Convolution and CNN-Based Choreography](#background-ddc)  
- [Limitations of the CNN Approach and the Case for Tokenization](#limitations-cnn)  
- [Tokenizing Music: From Audio to Musical Events](#tokenizing-music)  
- [Encoding Musical Structure with Transformers (MT3 & Music Transformer)](#encoding-structure)  
- [Sequence-to-Sequence Architecture for Music-to-Dance Generation](#seq2seq-architecture)  
- [Data Preparation and Preprocessing](#data-preparation)  
- [Evaluation Metrics and Comparison](#evaluation-metrics)  
- [Beyond Rhythm Games: Applications in VR and Artistry](#beyond-rhythm-games)  
- [Conclusion and Future Outlook](#conclusion)  

<a name="background-ddc"></a>
## Background: Dance Dance Convolution and CNN-Based Choreography

**Dance Dance Convolution (DDC)** by Donahue et al. (2017) was one of the first systems to automatically generate DDR dance charts from music. In DDR, choreographers create *step charts* — sequences of arrows (up, down, left, right) with precise timings that correspond to musical beats and phrases. Manually authoring these charts is time-consuming and requires skill, motivating research into automation. DDC framed this as a learning problem: given an audio clip, predict a sequence of dance steps that match the music.

**How the original model worked:** DDC used a two-stage approach: **(1) a CNN-based audio encoder** to extract features from the song, and **(2) an RNN (LSTM) decoder** to produce the sequence of steps. The input was the song’s spectrogram, segmented into short time windows (frames). The CNN learned to detect rhythmically salient audio patterns (like beats or musical onsets) from these frames. The RNN then integrated information over time, outputting a sequence of dance steps. In practice, the problem was further split into sub-tasks:
- **Step Placement:** First, decide *when* to place a step. This was treated as a binary classification for each time frame (step vs. no-step). A CNN or C-LSTM (CNN + LSTM) predicted whether each short time window contains a dance step.
- **Step Selection:** Next, given that a step occurs at a certain time, decide *which arrow(s)* or step pattern to activate. This was handled by a conditional model (another LSTM) that outputs the specific foot arrows, possibly using the audio features and the previous steps as context.

**Results:** DDC demonstrated **playable, coherent DDR choreographies** generated by AI. The best-performing variant was the CNN+LSTM (“C-LSTM”) model, which outperformed simpler baselines like logistic regression or MLPs. For example, on a dataset by expert step-chart author "Fraxtil", the C-LSTM achieved high precision/recall (F-score) in placing steps. Qualitatively, the generated charts mimicked human-designed patterns reasonably well. DDC proved that neural networks can capture the relationship between audio beats and dance steps.

Despite this success, the authors noted several **challenges and limitations** in their CNN/RNN-based approach, which hinted at where improvements could be made. Before diving into a new Transformer-based solution, let's examine what the limitations were and why a fresh perspective could help.

<a name="limitations-cnn"></a>
## Limitations of the CNN Approach and the Case for Tokenization

While the CNN+RNN model of DDC broke ground, it had **inherent limitations** due to how it represented the problem. These limitations motivate moving from a **frame-by-frame CNN approach** to a **tokenized sequence modeling approach**:

- **Frame-wise prediction & class imbalance:** DDC’s step placement predicted on each small time frame whether a step occurs. But dance steps are **sparse** in time – a song might have a step every half-second or so, meaning hundreds of intervening frames with “no step.” This led to a severe class imbalance: far more negative examples (no step) than positive. The model tended to bias towards "no step," and training was tricky. *Tokenization can eliminate this issue* by representing only the meaningful events (steps) rather than every empty moment. Instead of asking “step or not at this 20ms slice?”, a sequence approach asks “when is the next step?” which focuses learning on actual events.

- **Local scope vs long-term structure:** CNNs look at a fixed window of audio (DDC used ~0.5s windows) to decide on a step. This is good for catching local cues like drum hits, but **music and choreography have structure over longer timescales** – motifs repeat, intensity rises and falls over measures, etc. The original DDC did add an LSTM to capture some longer context. Still, an LSTM compresses history into a hidden state, which might forget details over long sequences. Donahue et al. observed that **DDR charts exhibit long-range dependencies** (e.g. patterns that span multiple measures) and that sequence models (LSTMs) outperformed pure CNNs because they could exploit this structure. However, even LSTMs have limits: they have to carry information in a single vector, which can make remembering distant events difficult. In contrast, a **Transformer** with a token sequence can potentially look back arbitrarily far using attention, remembering that “the chorus at 1:30 has similar melody as at 0:45” and thus should have similar steps.

- **Limited representation of musical structure:** The CNN operates on raw spectrogram amplitudes. While it can learn that, say, a spike at 100 Hz means a kick drum, it doesn’t explicitly *know* about musical constructs like beats, tempo, or note events. It’s treating the input as a continuous signal. A **tokenized representation** of the music (for example, a sequence of **discrete musical events like beats or notes**) could make the structure more explicit. By feeding the model *musical tokens* (e.g. “Kick drum on beat 1, snare on beat 2, piano melody note C on beat 2.5”), we provide a higher-level, compact sequence for the model to learn from. This can improve learning of rhythm and melody correspondences to dance moves, as the model deals with a **sequence of meaningful events** rather than thousands of raw audio frames.

- **Two-stage pipeline complexity:** The original pipeline separated *when* to step from *what* step to perform. This was partly to handle sparsity and make the task manageable. But splitting can lead to compounding errors (if step placement misses a beat, step selection can’t fix it) and might prevent joint optimization of timing and choreography patterns. A **unified sequence-to-sequence model** can potentially generate the full chart in one go, deciding both timing and step patterns together. Recent research has indeed suggested re-formulating chart generation as a single sequence prediction problem to avoid dealing with frame-level imbalance.

In summary, the CNN-based approach was limited by **frame-level processing, local focus, and implicit musical understanding**. A tokenized sequence approach addresses these by:
- Modeling only the **sequence of events** (notes/beats and steps) – no more wasted effort on empty time frames.
- Using powerful **Transformer architectures** to capture **long-range dependencies** in those event sequences (music and dance).
- Incorporating an **explicit representation of musical structure** (via tokenization) to guide the choreography generation.

Next, we discuss how we can represent music in a tokenized form and leverage state-of-the-art music sequence models to encode the input song in a way that’s much more amenable to learning a mapping to dance moves.

<a name="tokenizing-music"></a>
## Tokenizing Music: From Audio to Musical Events

**What does it mean to tokenize music?** In natural language processing, we convert sentences into tokens (words or subwords) before feeding them to a Transformer. We can do something analogous for music: convert a raw audio waveform or spectrogram into a sequence of **musical tokens** that capture the salient events in the music. For example, tokens might represent:
- **Notes and rhythms:** e.g. “Note-On (pitch=C4, velocity=80) at time 1.0s”, “Note-Off at 1.5s”
- **Beat positions:** e.g. “Beat 1 of measure 5”, “16th-note time step”
- **Instrument or drum events:** e.g. “Kick drum event at time X”, “Snare drum at time Y”

Such a sequence of events can be extracted from symbolic music (like a MIDI file). But if we only have audio, we can use **automatic music transcription** to get a symbolic approximation of the audio. This is where models like **MT3** come into play.

**MT3 (Multi-Task Multitrack Music Transcription)** is a Transformer-based model from Google Magenta that converts audio into a sequence of MIDI-like tokens. It treats transcription as a seq2seq problem: spectrogram frames go in, and a sequence of note events (with pitches, instruments, and timings) comes out. The vocabulary is a “MIDI-like” language of music. MT3 showed that **off-the-shelf Transformers can transcribe music as well as specialized CNN/RNN architectures**, reinforcing the idea that a generic sequence model can understand audio when trained on appropriate data. In practical terms, we could use a pretrained MT3 to **transcribe an input song into a sequence of tokens** that represent the music.

For example, consider a measure of a dance track:
- The audio might contain a bass drum on beats 1 and 3, a snare on 2 and 4, and a synth riff of certain notes.
- MT3 (or a similar model) could output a token sequence like:  
  `[Start]`, `Time=0.0s`, `DrumKick`, `Time=0.5s`, `DrumSnare`, `Time=1.0s`, `DrumKick`, `Time=1.0s`, `NoteOn_C4`, `Time=1.25s`, `NoteOn_E4`, `Time=1.5s`, `DrumSnare`, `Time=1.75s`, `NoteOff_C4`, ... `[End]`.  
  (This is a simplified illustration – actual tokenization schemes are carefully designed, as discussed below.)

Instead of raw time in seconds, we often quantize time into musical intervals (e.g., 48 ticks per quarter note as in the new research). In a **beat-aligned tokenization**, time steps are relative to the song’s tempo grid. For instance, tokens might indicate positions like “at the 3rd 16th-note of beat 10”. This ensures that the sequence length stays manageable and aligned across different tempos.

**Tokenization schemes:** Recent work on symbolic music generation has established some common representations we can borrow:
- **MIDI-like (event-based):** A sequence of events like `Note-On (pitch)`, `Note-Off (pitch)`, and `Time-Shift (Δt)` tokens. This was used in **Performance RNN** and the original **Music Transformer**. It captures expressive performance including timing and velocity of each note.
- **REMI (REpresentation of MIdi):** Introduced for **Pop Music Transformer**, this adds *beat* and *position* tokens to explicitly mark rhythm, making it easier to learn music with a metrical structure. For example, a bar might start with a `Bar` token, then within the bar, `Position_1/16`, `Position_2/16`, etc., along with note events. This improves the model’s awareness of where it is in the measure.
- **Compound Words:** Some recent tokenizations combine multiple musical attributes into single tokens for efficiency. For instance, **MT3’s vocabulary** encodes a note’s pitch, instrument, and other attributes in one token ID.
- **Chart tokens:** For choreography specifically, one can define a tokenization of dance steps. The new approach by Lee et al. 2023 (which we’ll reference shortly) does exactly this: Each **dance step event** becomes two tokens – a *time token* (discrete time position in a measure) and an *action token* (which arrows are pressed). For DDR with 4 arrows, they enumerate all possible combinations (including holds), resulting in about 80 action tokens.

**Why tokenization helps:** By converting both the **music and the choreography into sequences of tokens**, we turn the problem into a **sequence-to-sequence learning task** – essentially like translation: *music sequence in, dance sequence out*. This has several advantages:
- We **focus the model on meaningful events**. The sequence is much shorter than frame-by-frame audio, and every token carries useful information (no meaningless “no step” frames).
- It enables the use of **Transformer architectures** which excel at modeling sequences with complex dependencies. The model can learn relationships like “a snare drum hit often corresponds to a down-arrow step after a half-beat delay,” etc.
- It **makes the music structure explicit**. If we include bar and beat tokens or relative timing, the model knows, for example, where the downbeat is, so it can place a big jump or a special move on that strong beat.
- It also simplifies *output* representation. Instead of predicting multiple signals (e.g., 4 arrow states at every time slice), the model just generates one token at a time. This can naturally handle varying step densities – if a section has no steps, the model simply outputs a longer gap (or a future time token) rather than a bunch of “no-step” outputs.

In the next section, we’ll look at how **Transformer models like Music Transformer and MT3** can be utilized to encode these token sequences and capture musical structure, forming the backbone of our new choreography-generation model.

<a name="encoding-structure"></a>
## Encoding Musical Structure with Transformers (MT3 & Music Transformer)

Transformers have proven extraordinarily effective at modeling sequences in both language and music domains. Two influential models in the music domain are **Music Transformer** (Huang et al. 2018) and the **MT3 transcription model**. We can draw inspiration from both for our choreography task.

**Music Transformer** is an autoregressive Transformer that generates musical sequences (like polyphonic piano performances) with long-term coherence. It introduced a novel **relative self-attention** mechanism to better capture musical structure. The key insight was that music has repeating patterns and motifs that might occur at different positions in the timeline; using **relative positional encodings** allows the model to recognize when a pattern repeats, regardless of absolute time. This enabled Music Transformer to generate pieces with convincing long-range structure (e.g., a theme introduced in the beginning reappearing at the end). In contrast to an LSTM that compresses history in a hidden state, the Transformer can directly **attend to any past event** it needs. This is ideal for our problem: a dance pattern that worked for the first chorus should be referenced and re-used (with variation) when the chorus returns. A vanilla CNN or LSTM might struggle to make that connection; a Transformer can learn to **attend back to the earlier chorus tokens** and copy or transform them for the later chorus.

In our approach, we can use a **Transformer encoder** to encode the input music sequence (tokenized as described). By using techniques from Music Transformer, such as relative attention, the encoder can effectively internalize the music’s structure:
- It can represent the **context of each musical event** (e.g., this note is in the middle of a fast arpeggio vs. a long held chord).
- It can learn what events are **important for dance** (perhaps drum beats and strong basslines are very relevant, while subtle background pads might be less so).
- With relative attention or explicit rhythmic tokens, it can generalize patterns to longer sequences than it saw in training, an important factor since songs can be several minutes (hundreds of tokens long).

Meanwhile, **MT3**, as mentioned, is essentially a **Transformer-based seq2seq model** (built on the T5 text-to-text framework) that takes spectrogram patches as input (via an encoder) and outputs a sequence of music tokens (via a decoder). For our purposes, there are two ways to leverage MT3:
1. **Preprocessing / Feature Extraction:** Use MT3 as a front-end to convert audio to a token sequence (transcription). The downstream model then doesn’t have to deal with raw audio at all – it gets a “score-like” representation from MT3. This could dramatically simplify learning, since transcription is a solved sub-problem (to an extent) and we can focus on mapping notes to steps.
2. **Architectural inspiration / Weight initialization:** Since MT3 is already a trained audio-to-sequence model, we could fine-tune it for chart generation. For example, take an MT3 model and retrain its decoder to output dance tokens instead of note tokens. Essentially, we’d be treating choreography generation as a different “language” that the model translates the audio into. The encoder (which processes audio spectrogram) could be largely reused from MT3’s pretrained state, giving the model a strong understanding of musical audio. The decoder would learn the “language of dance” in place of the language of notes. This approach leverages transfer learning from music transcription to choreography – a fascinating direction because hitting arrows in DDR is in some ways a simplified, constrained “performance” synced to music.

Whether we explicitly use a pretrained MT3 or not, the concept is that our model will involve a **Transformer encoder that processes the input music in token form**. If not using a separate transcription step, this encoder could directly ingest a sequence of spectral or low-level tokens (e.g., quantized audio embeddings) to encode the audio. However, using a **musically-informed token representation plus a music-trained Transformer** gives a head start by encoding musical knowledge.

To sum up:
- **Music Transformer** provides the capability to capture *long-term musical dependencies and structure* in a sequence model, which is crucial for aligning choreography with repeated sections and global music dynamics.
- **MT3** provides a way to go from *raw audio to structured tokens* and demonstrates that a Transformer can serve as the audio encoder for a sequence output. It essentially shows “audio in, tokens out” is feasible with Transformers, meaning we don’t necessarily need CNNs for feature extraction if we have enough data or a pre-trained model.

With these pieces (music tokenization and Transformer encoders/decoders) in mind, we can design a new **sequence-to-sequence model for music-to-dance generation**. In the next section, we outline how the architecture might look and consider a few possible variants.

<a name="seq2seq-architecture"></a>
## Sequence-to-Sequence Architecture for Music-to-Dance Generation

The task now is essentially **conditional sequence generation**: given a sequence of music tokens (condition), generate a sequence of dance tokens (output). This naturally lends itself to an **encoder–decoder Transformer architecture**, much like machine translation or sequence transduction tasks.

**Proposed Model Architecture:**  
- **Music Encoder:** A Transformer encoder that takes the tokenized music sequence as input. It produces a set of hidden representations that encode the musical context at each token position. This could be a standard Transformer encoder (possibly with relative positional encoding to capture rhythm patterns). If using raw audio directly, this encoder could be preceded by a small CNN or downsampling to turn the spectrogram into initial tokens, but ideally we use musical tokens so the encoder is purely attention-based.
- **Dance Decoder:** A Transformer decoder that generates the dance sequence token-by-token. It attends to the encoder’s output (music context) through **cross-attention**, and attends to its own past outputs through self-attention (to maintain sequence consistency). The decoder is trained to produce the next dance token given the music and the tokens of the dance sequence generated so far (teacher forcing during training).

This design is illustrated conceptually by recent work (Lee et al. 2023) which similarly uses an encoder-decoder Transformer for chart generation. In their pipeline, a chunk of the spectrogram (covering a few beats of music) is encoded, and the decoder outputs the step tokens for the next few beats. The process iterates over the song. Our approach aligns with this, though we can imagine some alternatives as well.

Possible variants or enhancements to the architecture:
- **Bidirectional Music Context:** The encoder by default provides context of the entire input sequence. This means the decoder can look at any part of the song’s music while generating a step at a given time. This is powerful – e.g., the model can use future music context to decide current choreography if needed – but one could also restrict it to causal (past) context if aiming for online generation. Typically, for off-line choreography generation, having full context is fine (the entire song is known).
- **Autoregressive vs Non-autoregressive decoding:** The standard approach is autoregressive – generate one token at a time. This ensures the model can condition each step on the previously generated steps (important for not outputting physically impossible moves like two conflicting arrows at once if that’s not allowed). Non-autoregressive decoding (predicting the whole sequence in parallel) is probably not suitable here because dance sequences have internal dependencies (e.g., certain moves follow others).
- **Difficulty and Style Conditioning:** We might include additional inputs to the encoder or decoder to control *what kind of choreography* to generate. For example, a **difficulty level embedding** can be added (as done in prior works) to tell the model whether to produce a simple or dense step chart. Similarly, one could imagine a “style” token (if we want the model to mimic a particular choreographer’s style or a particular dance genre). This could be concatenated to the encoder input or as a special token that the decoder attends to. In an encoder-decoder setup, it’s easy to feed such global conditioning by, for instance, prepending a token to the music sequence like `<Difficulty=Hard>`.
- **Length and Memory Considerations:** Full songs can be long (hundreds of beats). Transformers have quadratic cost in sequence length, so training on full songs end-to-end might be difficult. One strategy is to **segment the song into sections** (e.g., 4 or 8-beat segments) for training, as done by Lee et al. where they used 4-beat segments. The model can be trained to generate a segment of choreography from a segment of music, possibly with some overlap or using the previous segment’s last few tokens as context (sliding window). During inference, it would generate segment by segment, feeding the next segment of music and the last part of the previous dance output to continue the sequence. This keeps the sequence lengths within a manageable range while still allowing the model to handle long songs through an iterative process.
- **Alternate: Single-Sequence Model:** As an alternative to the classic encoder-decoder, one could experiment with a single Transformer that takes a concatenated sequence of “music + dance” tokens. For example, the input could be a sequence like `[MusicToken, MusicToken, ..., <SEP>, DanceToken, DanceToken, ...]` and the model is trained to output the dance tokens given the music tokens as context (perhaps using masking so it doesn’t try to predict the music). This is more akin to a language model prompt style (like GPT feeding context). While possible, it’s a bit more awkward to implement and typically the encoder-decoder formulation is cleaner for conditional generation. Thus, we favor the encoder-decoder design for clarity.

**Training the seq2seq model:** We would train this model on a dataset of songs paired with choreographies (more on data in the next section). The loss is usually the cross-entropy between the predicted token distribution at each step and the ground-truth next token. We can also evaluate **per-token accuracy** during training. Techniques like teacher forcing (providing the true previous token during training) are used, and at inference we sample autoregressively. If the dataset is large, training from scratch is feasible, but if limited, leveraging pretrained components (like using a pretrained music encoder from MT3 or a pre-trained decoder on some proxy task) could help.

**Ensuring realistic outputs:** One challenge is that not every sequence of dance tokens is physically possible or sensible (e.g., certain rapid step combinations might be too fast to perform). The original DDC had to ensure things like “not turning the player backwards” which they addressed via post-processing or constraints. In a token-generation framework, some of these constraints could be learned (the model hopefully learns not to output bizarre combinations if it never saw them in training). We could also incorporate constraints via the token design (for example, encoding holds and releases explicitly to avoid impossible overlaps). If needed, a **constraint mask** can be applied during decoding to prevent illegal token combinations (similar to how language models can prevent invalid sequences). But ideally, a well-trained model on a good dataset will implicitly respect these rules.

To recap, our architecture of choice is an **Encoder-Decoder Transformer** that translates a sequence of music tokens into a sequence of dance tokens. This directly addresses the sequence modeling nature of the task and can capture complex alignments between music and choreography. Now, let’s talk about the practical steps to get the data into this form and ready for training.

<a name="data-preparation"></a>
## Data Preparation and Preprocessing

Switching to a tokenized seq2seq approach requires careful **data preprocessing**. We need to prepare both the input (music) and output (dance) sequences in a suitable format for training our model. Here’s a breakdown of the key steps and considerations:

**1. Collect Paired Music–Choreography Data:**  
First, we need a dataset of songs with corresponding dance choreographies. In the DDR context, we have datasets like **Fraxtil’s custom charts and ITG (In The Groove) charts** used in DDC. However, those were relatively small (on the order of a few hundred songs). For training a complex model like a Transformer, more data is beneficial. Recent studies have turned to larger datasets, for example **o!mania (osu!mania) 4-key charts** – Lee et al. collected over 14,000 charts for 4-key rhythm game songs. For our approach, we’d ideally gather a large corpus of songs with their step charts, possibly from community contributions or game databases. Each data point should have the raw audio (or a way to get musical tokens) and the timeline of dance steps (with timestamps and actions).

**2. Music Transcription / Tokenization:**  
For each song, we convert the audio into a sequence of music tokens. There are a few options:
   - **Use a Pre-trained Transcription Model:** Run an automatic music transcription (like MT3) on the audio to get a MIDI-like event sequence. This gives note events (pitches, onset times, durations) and possibly drum events if the model supports multi-instrument transcription. After transcription, we quantize the events to a tempo grid. It’s important to estimate the song’s tempo and beat alignment; this can be done via existing Music Information Retrieval (MIR) tools or by analyzing the transcription (e.g., detect periodicity of percussion).
   - **Beat and Onset Features:** If full transcription is too complex or error-prone for some genres, a simpler approach is to detect beats and important onsets. For example, use a beat tracker to get a sequence of beat times and a spectral onset detector to find points where new sounds occur. These can be discretized into a rhythmic grid (e.g., at a resolution of 1/16 or 1/24 of a beat). The presence of energy in certain frequency bands at those points can be tokenized (e.g., token for “low-frequency onset at this 16th-note” might indicate a drum kick).
   - **Direct Score Input:** In cases where the song is available as a MIDI or musical score (not common for arbitrary songs but possible for game music), we can directly use that symbolic data as tokens.

   Regardless of method, we aim to produce a sequence of tokens such as:  
   `Bar_Start, Beat=1, Sixteenth=0, Note=C4, Note=E4, Beat=1, Sixteenth=2, Note=G4, ...` etc., up to an `End_of_Song` token. We also include **timing tokens**. One scheme (inspired by REMI) is to use explicit tokens for the passage of time: e.g., `Beat` and `Sixteenth` tokens that indicate time positions. Another (inspired by Lee et al.) is to use a combined time token that counts subdivisions from a reference point. For instance, they had time tokens 0–95 representing positions within two measures (since they used 48 subdivisions per beat, 4 beats, so 192 subdivisions = 0–191, but they chunked into half). The exact scheme can vary, but the goal is that the **music token sequence encodes what happens and when** in a discretized form.

**3. Choreography Tokenization:**  
We also need to represent the output dance chart as tokens. This involves:
   - **Selecting a time resolution:** Typically, DDR steps are aligned to musical rhythm (quarter notes, eighth notes, etc). We should choose a resolution fine enough to capture any possible step timing. DDC used 27 ms frames ~ (1/16th note at 138 BPM). Lee et al. chose 1/48 of a beat as their hop size, which is very fine (3 * 16th notes, i.e., triplet 16ths). A resolution of 1/24 or 1/48 beat is usually sufficient for almost all step timings in rhythm games. 
   - **Time tokens for steps:** Similar to music, we introduce time-step tokens for the dance sequence. One simple way: for each step event, include a token that says how much time (in subdivisions) has passed since the last step. This would be like a `TimeShift` token (e.g., `Time+3` meaning the next step happens 3 ticks later). Alternatively, use absolute time tokens relative to a measure or bar (like Lee et al.’s approach of a time token 0–95 within each segment). If we generate one step at a time, relative timing works well.
   - **Action tokens for steps:** We define tokens for the actual dance action. In DDR’s case, an action is which arrows are stepped on at that time. With 4 arrows, a dancer can press one or two at once (two is a jump), or hold arrows. One approach is to have separate tokens for each arrow state (like “Left arrow pressed”), but it’s more efficient to encode the combination as one token. For example, define an ID for each of the 2^4 = 16 possible combinations of arrow presses (including holds) – but excluding the “none” because we don’t output when no step. Also we must incorporate hold starts and stops. Lee et al. ended up with 80 possible “chart actions” as token vocabulary, encoding single steps, jumps, hold starts/ends, etc. We can adopt a similar enumeration. So an action token might be, say, `Action_L+D` meaning a jump on Left and Down arrows, or `Action_U(hold)` meaning start a hold on Up arrow.
   - **Sequence construction:** The dance sequence can then be formed as an alternating series of time tokens and action tokens: e.g., `Time+2, Action_U, Time+1, Action_R+hold, Time+0, Action_R(release), Time+3, Action_U+D`. In this example, the model placed an Up arrow, then immediately a Right arrow hold (with 1 tick gap), then released it with no gap (Time+0 means the release happened at the same timestamp, which implies a hold duration), then 3 ticks later a jump Up+Down.

   We will also include a special end-of-sequence token to mark that the chart (for that song) is finished.

**4. Aligning Music and Dance Sequences:**  
During training, the model needs to learn to align the music tokens with the appropriate dance tokens. Because we feed the entire music sequence to the encoder and generate dance from the decoder, explicit alignment is not forced (the model will learn it implicitly by cross-attention). However, it helps to ensure that the two sequences use the **same timeline reference**. This typically means if we quantize music events to a certain grid (say 1/48 beat), we should use the same grid for dance steps. By doing so, a given time-step in the music and dance correspond. In practice, one might divide the song into segments (e.g., 8 beats). Ensure that each segment of music tokens covers the same time span as the corresponding segment of dance tokens used as the training target. Lee et al. did this by always feeding 4 beats of music and generating the next 2 beats of dance, effectively offsetting segments and stitching them. We can simplify by feeding a segment of music and having the target be the dance in that segment; or feed whole song and generate whole song – but that’s heavy for training.

   Another subtle point: **tempo changes**. If a song has variable tempo or time signature, aligning on a unified grid is tricky. One approach is to warp time to a constant tempo grid (beat normalization) by scaling the spectrogram hop size dynamically. This is advanced, but for simplicity we might avoid songs with tempo changes, or include tempo tokens to inform the model of tempo shifts.

**5. Data Augmentation (optional):**  
To help the model generalize, we can augment the data:
   - **Audio augmentations:** pitch-shift or time-stretch the audio slightly (and correspondingly shift the dance timings) to increase variety.
   - **Chart augmentations:** Since dance charts have some symmetry, one could mirror left/right in the choreography (swap left and right arrows throughout). This produces a valid new chart for the same song (just mirrored). The music is unchanged, but the dance tokens are transformed. This doubles data and teaches the model that left vs right can sometimes be symmetric responses to music (useful for generalization).
   - **Subset selection:** Possibly generate simpler charts from complex ones by algorithmically removing some steps (to simulate easier difficulties). Though if we have real labeled difficulties, that’s better.

**6. Preparing Input-Output Pairs:**  
Finally, structure the training pairs. Each training example could be a full song or a segment:
   - For full-song training: input is the full token list of music, output is full token list of dance. This requires handling long sequences but keeps global context intact.
   - For segment training: break songs into overlapping segments (to allow the model to learn transitions). For example, take 8-beat windows with a stride of 4 beats (so each segment shares half with the next). The input is 8 beats of music tokens, and output is 8 beats of dance tokens. We can condition the first few beats of dance in each segment with previous output during generation to ensure continuity.
   - Include any metadata in the input representation (like difficulty level as mentioned).
   - Shuffle and create a large number of such pairs for training.

By the end of preprocessing, we have a dataset where each sample looks like:
- **Encoder input:** `[MusicToken_1, MusicToken_2, ..., MusicToken_M]` (plus maybe a difficulty token).
- **Decoder target:** `[DanceToken_1, DanceToken_2, ..., DanceToken_N]`.

With this dataset, we can train our seq2seq model to learn the mapping. The heavy lifting of parsing audio into meaningful bits is largely handled by our tokenization/transcription steps, which should make the learning task more about *mapping patterns* than *discovering the concept of beat from scratch* (which the CNN had to do to some extent).

Having prepared our data and model, we next consider how to evaluate this new approach and compare it to the original DDC baseline.

<a name="evaluation-metrics"></a>
## Evaluation Metrics and Comparison

Evaluating generated choreography has facets of both objective measurement and subjective judgment (how fun or creative is the dance?). For fair comparison with the original model, we can adopt similar **objective metrics** as used in DDC, and then discuss additional evaluation criteria.

**Metrics used in the original DDC:**
- **Perplexity (PPL):** In DDC, perplexity was used to evaluate the sequence models (lower perplexity indicates the model predicts the ground-truth sequence with higher confidence). They computed PPL per frame or per step. In our seq2seq context, we can measure the **per-token perplexity** of the dance sequence under our model. This tells us how well the model has learned the distribution of valid dance sequences given the music.
- **Precision, Recall, F-score:** For step placement, DDC used precision-recall curves and F1-score to measure how well the model identified the correct moments for steps. We can do a similar event-based evaluation: treat each dance step as a timed event and compare the set of predicted events to the ground truth events. A predicted step is considered correct if it falls within a small tolerance of an actual step timing (e.g., within 50ms or within the same quantized tick). Then compute:
  - *Precision:* fraction of predicted steps that matched real steps.
  - *Recall:* fraction of real steps that were predicted.
  - *F1-score:* harmonic mean of precision and recall.
  
  We can compute these for each song (chart) and average, or even break it down by segments of the song. **Micro-F1** (overall across all steps in the test set) was used in some comparisons. DDC also reported **per-chart F1** (they call F-score_c) and micro/global F1 (F-score_m).
- **AUC (Area Under Curve):** DDC reported area under the precision-recall curve as well, by varying the threshold for step placement. For our model, which directly produces discrete steps, this might be less applicable (we’re not thresholding a continuous output). AUC was more relevant when it was a frame-wise classification problem. In our case, precision/recall at the natural operating point (the model’s chosen steps) is probably sufficient.

**Metrics for sequence quality:**
- **Sequence Accuracy:** DDC measured per-token accuracy for step selection (how many of the predicted next-step tokens matched the ground truth). In our case, we can measure how many tokens in the entire output sequence match exactly the ground truth sequence. However, this is a harsh metric – because there could be many valid choreographies for a song, not just the ground truth. The model might generate a different but equally good sequence of steps. So exact match is usually too strict (except maybe for very deterministic tasks).
- **Edit Distance / Sequence Similarity:** One could use an edit distance between the predicted dance sequence and the ground truth sequence. This counts insertions, deletions, substitutions of step events. A smaller edit distance means the model output is closer to the human-designed chart. This can capture if the model is missing some steps (deletions) or adding extra (insertions).
- **Rhythmic alignment scores:** We could specifically measure how well the steps align with musical beats. For example, what fraction of steps occur on the downbeat, on strong beats versus offbeats, compared to the ground truth distribution. This assesses if the model learned to hit musically salient moments.
- **Pattern repetition / structure scores:** Since one selling point of the Transformer approach is capturing structure, we might evaluate whether the generated choreography exhibits similar sectional structure as the music. For instance, if a chorus repeats, did the model repeat a similar step pattern? We can use a self-similarity matrix: compare the dance sequence structure to the music structure. This is more experimental as a metric, but could be insightful in research.

**Human or Subjective Evaluation:** Beyond automated metrics, in a deployment scenario we care about how enjoyable or acceptable the choreography is:
- **Playability:** Does the chart avoid awkward or impossible moves? This could be evaluated by expert players or by checking certain heuristics (e.g., no consecutive jumps faster than X, no doublestep issues where the player’s foot would have to do a strange cross).
- **Fun factor / style:** Human players could rate the generated charts. Or compare side-by-side with human charts in a blind test: can players distinguish AI-generated from human-made? 
- **Coverage of difficulty:** We should ensure that if the model is asked to generate an “Easy” chart vs “Expert” chart, the outputs indeed differ in density and complexity. We can measure the number of steps per minute in the output as a proxy for difficulty and see if it matches the target.

In comparing **the new Transformer-based approach vs. the original CNN-based DDC**, we would likely look at:
- **Improvement in F1 or precision/recall:** The new model should ideally miss fewer steps and add fewer extraneous steps, raising these scores. Lee et al. (2023) found that their Transformer sequence model outperformed prior methods in micro-F1 significantly on standard datasets. For instance, after pretraining on a large dataset, their model achieved around 78–79% F1 on the DDR benchmark, versus ~75% for the DDC model. Such gains demonstrate the benefit of the new approach.
- **Perplexity reduction:** A lower perplexity indicates the model is more confident in the correct sequence. If our approach is modeling the sequence better, we expect perplexity per step to drop compared to an RNN’s perplexity in DDC.
- **Generalization to new songs:** We might test on songs very different from those in training (different genre). A robust model using musical tokens might handle a wider variety of music because it understands musical concepts, whereas a CNN might overtune to spectral patterns seen in training. If possible, using a **larger training set** (like many thousands of songs from osu!/StepMania) should give our model an edge in generalization. We could measure performance on a hidden set of songs from a different source to compare.
- **Ablation tests:** To justify tokenization, we could try a baseline where we feed raw spectrogram frames into the Transformer (like treat each frame as a token) and see if performance is worse. We expect that without the intelligent tokenization, the model might struggle or need more data. Another ablation is to remove relative position encoding or beat alignment and see if performance drops, indicating those contributed.

**Evaluation beyond numbers:** When it comes to real-world use, a model that generates *good* dance sequences might occasionally diverge from the ground truth chart (maybe adding a flourish that wasn’t in the human chart). So, quantitative metrics tell only part of the story. For research we’ll report them, but we’d also examine examples qualitatively:
- Show a snippet of a song’s waveform/spectrogram, and compare the ground truth steps vs model steps plotted against it (visually inspecting if the steps align to major musical beats).
- Possibly have expert rhythm game players try the generated charts and give feedback (this was outside the scope of DDC, but future work could do this).

All said, a comprehensive evaluation will demonstrate that the **Transformer-based tokenized model meets or exceeds the original CNN-based model on objective metrics**, and provides more **structurally coherent and musically aligned choreographies**. Next, we’ll explore how this approach might extend to other domains, unlocking new creative applications.

<a name="beyond-rhythm-games"></a>
## Beyond Rhythm Games: Applications in VR and Artistry

While DDR and similar rhythm games are a natural application for music-to-dance generation, the idea of **generating choreography from music** has broader implications. Our tokenization and Transformer approach can be adapted or serve as inspiration for various creative and interactive domains:

- **VR Dance Performances:** Imagine a VR application where a virtual avatar dances to any song the user plays. By extending our model, we could generate full-body dance motion for the avatar in real-time or precompute it for the track. In a VR dance game or experience, this could allow infinite content – any music track becomes a dance sequence. The principles are similar, but instead of discrete DDR arrows, the output tokens would represent **body joint movements**. We could tokenitize human poses (perhaps using motion-capture data of dancers) and train a model to go from music tokens to pose sequences. Some recent works have explored generating 3D dance motions from music, showing applications in animation, virtual performers (idols), and the **metaverse**. Our approach could enhance these by providing a strong music understanding via the Transformer encoder. One can envision pairing this with a game like Beat Saber or a VR concert where the choreography is AI-generated yet synchronized perfectly to the music.

- **Choreography Assistance for Creators:** Choreographers could use such a system as a creative assistant. For instance, given a piece of music, the model could suggest a sequence of dance moves (perhaps output as a storyboard or a sequence of step names or even a stick-figure animation). The choreographer can then modify or build upon it. This could be especially useful for quickly prototyping dance routines for performances or music videos. By broadening the output vocabulary, the model could generate more complex dance moves beyond simple steps – e.g., spins, jumps, arm movements – described in some notation. This enters the realm of *artistic choreography generation*, where the goal isn’t a game score, but an aesthetically pleasing dance. The model might be trained on datasets like music videos with corresponding dance annotations or motion capture of dancers. For example, a system might generate a ballet sequence to classical music, or hip-hop moves to a rap song, providing a starting point for human choreographers.

- **Educational Tools:** In dance education, such a model could be used to **visualize rhythm**. If a student provides a piece of music, the system could generate a simple dance routine marking the beats (kind of like an embodied metronome). Students could then practice timing by following the generated steps. Additionally, it could generate variations of a routine to help students practice improvisation and adaptability to music changes.

- **Rhythm Therapy and Fitness:** Beyond formal dance, music-driven movement generation can be applied in fitness apps or therapy. For example, an exercise app might generate dance-like aerobic routines to the user’s favorite song, keeping them engaged. Similarly, for physical therapy, certain movements synced to music could be generated to encourage patients to move in rhythmic patterns (since music can motivate movement).

- **Generative Art Installations:** We could incorporate this technology in interactive art. Picture an installation where any music played in the space causes a digital character or a cluster of abstract shapes to dance in sync – essentially creating a visual choreography to the sound in real-time. Our model’s output could drive animations or even robotic dancers. The fact that it’s token-based means it can be hooked into other systems (e.g., controlling a robot’s joint actuators could be triggered by tokens representing those joint movements).

It’s worth noting that moving to full 3D dance or other complex outputs will require richer representations. However, our fundamental approach – learning a mapping from music structure to movement structure – still applies. In fact, researchers in music-driven dance generation often use sequence models and increasingly Transformers to capture the alignment between audio and motion. The success of Transformers in text and music suggests they can handle the **cross-modal translation** from audio to motion too.

By demonstrating the approach on the constrained DDR domain (which has clear rules and discrete outputs), we build a foundation that can inspire these broader applications. The idea of treating it as a translation problem – from the language of music to the language of dance – is very powerful. As noted in a recent study, “the outcome of dancing generation techniques can be applied to various applications such as animation, virtual idols, metaverse, or dance education”. We are already seeing that happen, and with the improvements discussed (tokenization, Transformers), the possibilities are expanding.

<a name="conclusion"></a>
## Conclusion and Future Outlook

In this post, we explored how to evolve the original **Dance Dance Convolution** approach into a modern deep learning pipeline by leveraging **tokenized representations and Transformers**. We started with a review of DDC’s CNN-based model, understanding its achievements and where it falls short in capturing long-term sequential structure. By introducing a **tokenization of music (and dance steps)**, we transform the problem into a sequence-to-sequence task, allowing us to harness powerful models like Music Transformer and MT3 that excel at sequence modeling and transcription. 

Our proposed approach uses an **encoder-decoder Transformer** to map musical token sequences to choreography token sequences, providing several benefits:
- It inherently deals with sequences of events, thereby avoiding issues of sparsity and imbalance in frame-based predictions.
- It can capture repeating motifs and long-range dependencies in both music and dance, leading to choreographies that reflect the song’s structure (e.g., consistent patterns for verses, variations for the bridge, etc.).
- It’s flexible to condition on style or difficulty, and can generalize to different music genres by understanding music at a higher level than raw audio.

We discussed how to implement this, from data preprocessing (where transcription and alignment play a key role) to architectural considerations (taking inspiration from proven music models). We also outlined how to evaluate the model, expecting improvements in metrics like F1-score and observing more coherent alignment between music and steps compared to the original CNN approach. Early indications from related research are promising: sequence Transformers are already outperforming CNN-based models on rhythm game benchmarks.

Finally, we looked beyond DDR, envisioning applications in VR dance generation, choreography assistance, and interactive art. As **AI-generated dance** continues to develop, we anticipate a convergence of techniques from different domains: the discrete token approach from rhythm games, the motion learning from computer vision, and the music understanding from MIR. This interdisciplinary fusion could enable systems that not only hit arrows on a pad, but truly *dance* to the music with grace and creativity.

**Implications:** Adopting a tokenized seq2seq approach could lead to choreographies that are **more musically expressive and varied**. Rather than being limited by local audio cues, the model can learn higher-level correspondences – for example, knowing that a sudden silence in the music might correspond to a pause or a special move in the dance. It also opens the door to **user-controllable generation** (via tokens controlling style/difficulty), making the AI a tool for creative expression rather than just an automatic chart generator.

**Future Work:** There are several exciting directions to pursue:
- Training on larger, more diverse datasets (including 3D dance motion data) to push the limits of what the model can choreograph.
- Incorporating **feedback from human dancers** into the training loop (e.g., via preference learning) to optimize for “fun” or “naturalness,” which pure audio alignment metrics can’t capture.
- Exploring real-time inference, so that the model could potentially generate choreography on the fly as music plays, enabling live adaptive dance visuals.
- Extending the vocabulary to handle more complex dance notation, for genres like ballet or contemporary dance, bridging the gap between rhythm game steps and full body movement.

In conclusion, by moving from convolutional frames to a **Transformer “language model” of music and dance**, we gain a richer understanding of the problem and achieve more powerful results. The marriage of music tokenization and deep sequence modeling is a promising step towards AI that can truly **listen to the music and dance**. As we refine these techniques, we edge closer to the day where AI choreographers might collaborate with human artists to create performances that none could have conceived alone – a new dance of man and machine, in perfect sync with the music.

*References cited inline in Distill style.*
